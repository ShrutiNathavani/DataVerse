{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['The smell of fresh-baked bread fills the air',\n",
    "        'Learning a new language can be challenging but rewarding',\n",
    "        'The scientist conducted experiments to test her hypothesis',\n",
    "        'The book, which was published last year, became an instant bestseller',\n",
    "        'The long wait for the bus frustrated commuters, leading to complaints from many of them',\n",
    "        'The river wound its way down the canyon toward the sea'\n",
    "        'The Eiffel Tower is a famous landmark in Paris, France',\n",
    "        'In 1969, Neil Armstrong became the first person to walk on the moon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Shruti\n",
      "[nltk_data]     Nathavani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shruti\n",
      "[nltk_data]     Nathavani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords for english language\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make function to tokenize\n",
    "def tokenize(doc_text):\n",
    "    tokens=nltk.word_tokenize(doc_text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc_text):\n",
    "    cleaned_text = []\n",
    "    for words in doc_text:\n",
    "        if words not in stopwords:\n",
    "            cleaned_text.append(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have the stemming process \n",
    "def stemming_process(doc_tokens):\n",
    "    stemmer=nltk.stem.PorterStemmer()\n",
    "    stemmed_words=[]\n",
    "    for word in doc_tokens:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The smell of fresh-baked bread fills the air'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=corpus[0]    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'smell', 'of', 'fresh-baked', 'bread', 'fills', 'the', 'air']\n",
      "['The', 'smell', 'fresh-baked', 'bread', 'fills', 'air']\n",
      "['the', 'smell', 'of', 'fresh-bak', 'bread', 'fill', 'the', 'air']\n",
      "the smell of fresh-bak bread fill the air\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenize(text)\n",
    "print(tokens)\n",
    "cleaned_text=remove_stopwords(tokens)\n",
    "print(cleaned_text)\n",
    "stemmed_tokens=stemming_process(tokens)\n",
    "print(stemmed_tokens)\n",
    "new_text=\" \".join(stemmed_tokens)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the smell fresh-bak bread fill air', 'learn new languag challeng reward', 'the scientist conduct experi test hypothesi', 'the book , publish last year , becam instant bestsel', 'the long wait bu frustrat commut , lead complaint mani', 'the river wound way canyon toward seath eiffel tower famou landmark pari , franc', 'in 1969 , neil armstrong becam first person walk moon']\n"
     ]
    }
   ],
   "source": [
    "#preprocessing the corpus\n",
    "cleaned_corpus = []\n",
    "for docs in corpus:\n",
    "    tokens=tokenize(docs)\n",
    "    cleaned_tokens=remove_stopwords(tokens)\n",
    "    stemmed_tokens=stemming_process(cleaned_tokens)\n",
    "    new_text=\" \".join(stemmed_tokens)\n",
    "    cleaned_corpus.append(new_text)\n",
    "print(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorise the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(cleaned_corpus)\n",
    "doc_vectors=vectorizer.transform(cleaned_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 52)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x52 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 57 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1969', 'air', 'armstrong', 'bak', 'becam', 'bestsel', 'book',\n",
       "       'bread', 'bu', 'canyon', 'challeng', 'commut', 'complaint',\n",
       "       'conduct', 'eiffel', 'experi', 'famou', 'fill', 'first', 'franc',\n",
       "       'fresh', 'frustrat', 'hypothesi', 'in', 'instant', 'landmark',\n",
       "       'languag', 'last', 'lead', 'learn', 'long', 'mani', 'moon', 'neil',\n",
       "       'new', 'pari', 'person', 'publish', 'reward', 'river', 'scientist',\n",
       "       'seath', 'smell', 'test', 'the', 'toward', 'tower', 'wait', 'walk',\n",
       "       'way', 'wound', 'year'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_frame=pd.DataFrame(doc_vectors.toarray(),\n",
    "                          columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1969</th>\n",
       "      <th>air</th>\n",
       "      <th>armstrong</th>\n",
       "      <th>bak</th>\n",
       "      <th>becam</th>\n",
       "      <th>bestsel</th>\n",
       "      <th>book</th>\n",
       "      <th>bread</th>\n",
       "      <th>bu</th>\n",
       "      <th>canyon</th>\n",
       "      <th>...</th>\n",
       "      <th>smell</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>toward</th>\n",
       "      <th>tower</th>\n",
       "      <th>wait</th>\n",
       "      <th>walk</th>\n",
       "      <th>way</th>\n",
       "      <th>wound</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398689</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434734</td>\n",
       "      <td>0.234589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314187</td>\n",
       "      <td>0.378499</td>\n",
       "      <td>0.378499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.285235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153917</td>\n",
       "      <td>0.285235</td>\n",
       "      <td>0.285235</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285235</td>\n",
       "      <td>0.285235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1969       air  armstrong       bak     becam   bestsel      book  \\\n",
       "0  0.000000  0.398689   0.000000  0.398689  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000   0.000000  0.000000  0.314187  0.378499  0.378499   \n",
       "4  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.339245  0.000000   0.339245  0.000000  0.281603  0.000000  0.000000   \n",
       "\n",
       "      bread       bu    canyon  ...     smell      test       the    toward  \\\n",
       "0  0.398689  0.00000  0.000000  ...  0.398689  0.000000  0.215139  0.000000   \n",
       "1  0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.00000  0.000000  ...  0.000000  0.434734  0.234589  0.000000   \n",
       "3  0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.204244  0.000000   \n",
       "4  0.000000  0.34729  0.000000  ...  0.000000  0.000000  0.187403  0.000000   \n",
       "5  0.000000  0.00000  0.285235  ...  0.000000  0.000000  0.153917  0.285235   \n",
       "6  0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      tower     wait      walk       way     wound      year  \n",
       "0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.00000  0.000000  0.000000  0.000000  0.378499  \n",
       "4  0.000000  0.34729  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.285235  0.00000  0.000000  0.285235  0.285235  0.000000  \n",
       "6  0.000000  0.00000  0.339245  0.000000  0.000000  0.000000  \n",
       "\n",
       "[7 rows x 52 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity with Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Neil Armstrong\"\n",
    "tokens=tokenize(query)\n",
    "cleaned_tokens=remove_stopwords(tokens)\n",
    "stemmed_tokens=stemming_process(cleaned_tokens)\n",
    "cleaned_query=\" \".join(stemmed_tokens)\n",
    "query_vector=vectorizer.transform([cleaned_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x52 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_doc_query=cosine_similarity(doc_vectors,query_vector).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.47976552])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_doc_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 4, 3, 2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recommand top-k documents for a given query\n",
    "document_index=cosine_doc_query.argsort()[:-10:-1]\n",
    "document_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1969, Neil Armstrong became the first person to walk on the moon\n",
      "The river wound its way down the canyon toward the seaThe Eiffel Tower is a famous landmark in Paris, France\n",
      "The long wait for the bus frustrated commuters, leading to complaints from many of them\n",
      "The book, which was published last year, became an instant bestseller\n",
      "The scientist conducted experiments to test her hypothesis\n",
      "Learning a new language can be challenging but rewarding\n",
      "The smell of fresh-baked bread fills the air\n"
     ]
    }
   ],
   "source": [
    "for docs_recommend in document_index:\n",
    "    print(corpus[docs_recommend])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

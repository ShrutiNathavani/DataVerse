{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shruti Nathavani\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about_text = ('I am a Python developer currently'\n",
    "             ' working for a London-based Fintech'\n",
    "              ' company. I am interested in learning'\n",
    "              ' Natural Language Processing.')\n",
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a Python developer currently working for a London-based Fintech company.\n",
      "I am interested in learning Natural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 0\n",
      "am 2\n",
      "a 5\n",
      "Python 7\n",
      "developer 14\n",
      "currently 24\n",
      "working 34\n",
      "for 42\n",
      "a 46\n",
      "London 48\n",
      "- 54\n",
      "based 55\n",
      "Fintech 61\n",
      "company 69\n",
      ". 76\n",
      "I 78\n",
      "am 80\n",
      "interested 83\n",
      "in 94\n",
      "learning 97\n",
      "Natural 106\n",
      "Language 114\n",
      "Processing 123\n",
      ". 133\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "     print (token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'Python', 'developer', 'currently', 'working', 'for', 'a', 'London', '-', 'based', 'Fintech', 'company', '.', 'I', 'am', 'interested', 'in', 'learning', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "def customize_tokenizer(nlp):\n",
    "     # Adds support to use `-` as the delimiter for tokenization\n",
    "     return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                      suffix_search=suffix_re.search,\n",
    "                      infix_finditer=infix_re.finditer,\n",
    "                      token_match=None\n",
    "                      )\n",
    "\n",
    "\n",
    "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n",
    "custom_tokenizer_about_doc = custom_nlp(about_text)\n",
    "print([token.text for token in custom_tokenizer_about_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should\n",
      "it\n",
      "‘re\n",
      "others\n",
      "around\n",
      "above\n",
      "mostly\n",
      "’ll\n",
      "itself\n",
      "ourselves\n"
     ]
    }
   ],
   "source": [
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "   print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "developer\n",
      "currently\n",
      "working\n",
      "London\n",
      "-\n",
      "based\n",
      "Fintech\n",
      "company\n",
      ".\n",
      "interested\n",
      "learning\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "     if not token.is_stop:\n",
    "         print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raj raj\n",
      "is be\n",
      "helping helping\n",
      "organize organize\n",
      "a a\n",
      "developerconference developerconference\n",
      "on on\n",
      "Applications Applications\n",
      "of of\n",
      "Natural Natural\n",
      "Language Language\n",
      "Processing Processing\n",
      ". .\n",
      "He he\n",
      "keeps keep\n",
      "organizing organize\n",
      "local local\n",
      "Python Python\n",
      "meetups meetup\n",
      "and and\n",
      "several several\n",
      "internal internal\n",
      "talks talk\n",
      "at at\n",
      "his his\n",
      "workplace workplace\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "conference_help_text = ('Raj is helping organize a developer'\n",
    "     'conference on Applications of Natural Language'\n",
    "     ' Processing. He keeps organizing local Python meetups'\n",
    "     ' and several internal talks at his workplace.')\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "    print (token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "complete_text = ('J K is a Python developer currently'\n",
    "    'working for a London-based Fintech company. He is'\n",
    "    ' interested in learning Natural Language Processing.'\n",
    "    ' There is a conference happening on 21 June'\n",
    "     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number '\n",
    "    ' available . J is helping organize it.'\n",
    "    ' He keeps organizing local Python meetups and several'\n",
    "    ' internal talks at his workplace. J is also presenting'\n",
    "     ' a talk. The talk will introduce the reader about \"Use'\n",
    "     ' cases of Natural Language Processing in Fintech\".'\n",
    "     ' Apart from his work, he is very passionate about music.'\n",
    "     ' J is learning to play the Piano. He has enrolled '\n",
    "     ' himself in the weekend batch of Great Piano Academy.'\n",
    "     ' Great Piano Academy is situated in Mayfair or the City'\n",
    "     ' of London and has world-class piano instructors.')\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "          if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K', 'developer', 'currentlyworking', 'based', 'company', 'interested', 'conference', 'happening', '21', 'June', '2019', 'titled', 'Applications', 'helpline', 'number', 'available', 'helping', 'organize', 'keeps', 'organizing', 'local', 'meetups', 'internal', 'talks', 'workplace', 'presenting', 'introduce', 'reader', 'Use', 'cases', 'Apart', 'work', 'passionate', 'music', 'play', 'enrolled', 'weekend', 'batch', 'situated', 'Mayfair', 'City', 'world', 'class', 'piano', 'instructors']\n"
     ]
    }
   ],
   "source": [
    "# Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piano Academy 0 13 ORG Companies, agencies, institutions, etc.\n",
      "Mayfair 29 36 GPE Countries, cities, states\n",
      "the City of London 40 58 GPE Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "piano_text = ('Piano Academy is situated'\n",
    "     ' in Mayfair or the City of London and has'\n",
    "     ' world-class piano instructors.')\n",
    "piano_doc = nlp(piano_text)\n",
    "for ent in piano_doc.ents:\n",
    "     print(ent.text, ent.start_char, ent.end_char,\n",
    "           ent.label_, spacy.explain(ent.label_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
